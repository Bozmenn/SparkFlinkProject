version: '3'

services:
  spark-app:
    build: ./spark-batch-processing-plugin
    container_name: spark-batch-processing-plugin
    volumes:
      - ./raw-data:/app/data

  zookeeper:
    image: arm64v8/zookeeper:3.7.0
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: wurstmeister/kafka:latest
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092 #for clients
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "orders:1:1,products:1:1,output:1:1"

  data-producer:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      BATCH_INTERVAL: 100
      BATCH_SIZE: 100
      ORDERS_SOURCE_PATH: /data/orders.json
      ORDERS_TOPIC: orders
      PRODUCTS_SOURCE_PATH: /data/products.json
      PRODUCTS_TOPIC: products
    depends_on:
      - kafka
    volumes:
      - ./raw-data:/data

  jobmanager:
    image: flink:1.19.0-scala_2.12-java8
    ports:
      - "8081:8081"
    command: standalone-job --job-classname com.berkozmen.MainProcess [--jars /opt/flink/usrlib/flink-stream-processing-plugin_2.12-0.1.0-SNAPSHOT.jar]
    volumes:
      - ./flink-stream-processing-plugin/target/scala-2.12:/opt/flink/usrlib
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        parallelism.default: 2        

  taskmanager:
    image: flink:1.19.0-scala_2.12-java8
    depends_on:
      - jobmanager
    command: taskmanager
    volumes:
      - ./flink-stream-processing-plugin/target/scala-2.12:/opt/flink/usrlib
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 2
        parallelism.default: 2   


